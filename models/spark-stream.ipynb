{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Spark Streaming\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, sum as spark_sum\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import webbrowser\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from copy import copy\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level to control the amount of information printed\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkEDA\").getOrCreate()\n",
    "\n",
    "# Read CSV into a Spark DataFrame\n",
    "df = spark.read.csv('Telco-Customer-Churn.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Head ####################\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|MultipleLines   |InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract      |PaperlessBilling|PaymentMethod            |MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female|0            |Yes    |No        |1     |No          |No phone service|DSL            |No            |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |29.85         |29.85       |No   |\n",
      "|5575-GNVDE|Male  |0            |No     |No        |34    |Yes         |No              |DSL            |Yes           |No          |Yes             |No         |No         |No             |One year      |No              |Mailed check             |56.95         |1889.5      |No   |\n",
      "|3668-QPYBK|Male  |0            |No     |No        |2     |Yes         |No              |DSL            |Yes           |Yes         |No              |No         |No         |No             |Month-to-month|Yes             |Mailed check             |53.85         |108.15      |Yes  |\n",
      "|7795-CFOCW|Male  |0            |No     |No        |45    |No          |No phone service|DSL            |Yes           |No          |Yes             |Yes        |No         |No             |One year      |No              |Bank transfer (automatic)|42.3          |1840.75     |No   |\n",
      "|9237-HQITU|Female|0            |No     |No        |2     |Yes         |No              |Fiber optic    |No            |No          |No              |No         |No         |No             |Month-to-month|Yes             |Electronic check         |70.7          |151.65      |Yes  |\n",
      "|9305-CDSKC|Female|0            |No     |No        |8     |Yes         |Yes             |Fiber optic    |No            |No          |Yes             |No         |Yes        |Yes            |Month-to-month|Yes             |Electronic check         |99.65         |820.5       |Yes  |\n",
      "|1452-KIOVK|Male  |0            |No     |Yes       |22    |Yes         |Yes             |Fiber optic    |No            |Yes         |No              |No         |Yes        |No             |Month-to-month|Yes             |Credit card (automatic)  |89.1          |1949.4      |No   |\n",
      "|6713-OKOMC|Female|0            |No     |No        |10    |No          |No phone service|DSL            |Yes           |No          |No              |No         |No         |No             |Month-to-month|No              |Mailed check             |29.75         |301.9       |No   |\n",
      "|7892-POOKP|Female|0            |Yes    |No        |28    |Yes         |Yes             |Fiber optic    |No            |No          |Yes             |Yes        |Yes        |Yes            |Month-to-month|Yes             |Electronic check         |104.8         |3046.05     |Yes  |\n",
      "|6388-TABGU|Male  |0            |No     |Yes       |62    |Yes         |No              |DSL            |Yes           |Yes         |No              |No         |No         |No             |One year      |No              |Bank transfer (automatic)|56.15         |3487.95     |No   |\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "#################### Schema ####################\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: float (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "#################### Summary Statistics ####################\n",
      "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+-----------------+-----+\n",
      "|summary|customerID|gender|     SeniorCitizen|Partner|Dependents|            tenure|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|    MonthlyCharges|     TotalCharges|Churn|\n",
      "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+-----------------+-----+\n",
      "|  count|      7043|  7043|              7043|   7043|      7043|              7043|        7043|         7043|           7043|          7043|        7043|            7043|       7043|       7043|           7043|          7043|            7043|                7043|              7043|             7032| 7043|\n",
      "|   mean|      NULL|  NULL|0.1621468124378816|   NULL|      NULL| 32.37114865824223|        NULL|         NULL|           NULL|          NULL|        NULL|            NULL|       NULL|       NULL|           NULL|          NULL|            NULL|                NULL| 64.76169246059922|2283.300441385536| NULL|\n",
      "| stddev|      NULL|  NULL|0.3686116056100135|   NULL|      NULL|24.559481023094442|        NULL|         NULL|           NULL|          NULL|        NULL|            NULL|       NULL|       NULL|           NULL|          NULL|            NULL|                NULL|30.090047097678482|2266.771363107635| NULL|\n",
      "|    min|0002-ORFBO|Female|                 0|     No|        No|                 0|          No|           No|            DSL|            No|          No|              No|         No|         No|             No|Month-to-month|              No|Bank transfer (au...|             18.25|             18.8|   No|\n",
      "|    max|9995-HOTOH|  Male|                 1|    Yes|       Yes|                72|         Yes|          Yes|             No|           Yes|         Yes|             Yes|        Yes|        Yes|            Yes|      Two year|             Yes|        Mailed check|            118.75|           8684.8|  Yes|\n",
      "+-------+----------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace whitespace with null values in 'TotalCharges'\n",
    "df = df.withColumn('TotalCharges', regexp_replace('TotalCharges', r'\\s+', ''))\n",
    "\n",
    "# Convert 'TotalCharges' to float\n",
    "df = df.withColumn('TotalCharges', col('TotalCharges').cast(FloatType()))\n",
    "\n",
    "# columns_to_convert = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "\n",
    "# for item in columns_to_convert:\n",
    "#     df = df.withColumn(item, when(col(item) == 'No', 0).otherwise(1).cast(IntegerType()))\n",
    "\n",
    "# Function to check Spark DataFrame\n",
    "def check_spark_df(dataframe, head=10):\n",
    "    print('#'*20, 'Head', '#'*20)\n",
    "    dataframe.show(head, truncate=False)\n",
    "    \n",
    "    print('#'*20, 'Schema', '#'*20)\n",
    "    dataframe.printSchema()\n",
    "    \n",
    "    print('#'*20, 'Summary Statistics', '#'*20)\n",
    "    dataframe.describe().show()\n",
    "\n",
    "# Apply the function to the Spark DataFrame\n",
    "check_spark_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------+\n",
      "|TotalCharges_missing_count|TotalCharges_total_rows|\n",
      "+--------------------------+-----------------------+\n",
      "|                        11|                   7032|\n",
      "+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to check missing values in Spark DataFrame\n",
    "def missing_values_table_spark(dataframe, return_cols=True):\n",
    "    missing_cols = [col_name for col_name in dataframe.columns if dataframe.filter(col(col_name).isNull()).count() > 0]\n",
    "    \n",
    "    if not missing_cols:\n",
    "        print(\"No missing values found.\")\n",
    "        return []\n",
    "    \n",
    "    missing_count_expr = [spark_sum(col(col_name).isNull().cast(\"int\")).alias(f\"{col_name}_missing_count\") for col_name in missing_cols]\n",
    "    total_rows_expr = [spark_sum(col(col_name).isNotNull().cast(\"int\")).alias(f\"{col_name}_total_rows\") for col_name in missing_cols]\n",
    "    \n",
    "    missing_data = dataframe.agg(*missing_count_expr, *total_rows_expr)\n",
    "    \n",
    "    missing_data.show()\n",
    "\n",
    "    if return_cols:\n",
    "        return missing_cols\n",
    "\n",
    "# Apply the function to the Spark DataFrame\n",
    "na_columns_spark = missing_values_table_spark(df, return_cols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(0, subset=[\"TotalCharges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = [col_name for (col_name, col_type) in df.dtypes if col_type in ('int', 'double')]\n",
    "numeric_cols.append(\"Churn\")\n",
    "\n",
    "# Convert \"yes\" to 1 and \"no\" to 0\n",
    "df = df.withColumn(\"Churn\", when(df[\"Churn\"] == \"yes\", 1).otherwise(0))\n",
    "\n",
    "# Calculate correlations with the target variable for numeric columns\n",
    "correlation_with_target = (\n",
    "    df.select(numeric_cols)\n",
    "    .toPandas()\n",
    "    .corr()\n",
    "    .loc[:, 'Churn']\n",
    "    .abs()\n",
    ")\n",
    "\n",
    "# Get the top 15 variables with the highest absolute correlations\n",
    "top_corr_vars = correlation_with_target.sort_values(ascending=False).head(40).index\n",
    "\n",
    "# Create a correlation matrix for the selected variables\n",
    "correlation_matrix = df.select(*top_corr_vars).toPandas().corr()\n",
    "\n",
    "# Create a heatmap using Plotly\n",
    "fig = px.imshow(\n",
    "    correlation_matrix,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    title='Top 15 Correlations with Target Heatmap (Numeric Columns Only)'\n",
    ")\n",
    "\n",
    "# Adjust the figure size\n",
    "fig.update_layout(width=1600, height=1600)\n",
    "\n",
    "fig.write_html(\"correlation_heatmap.html\")\n",
    "\n",
    "# Specify the path to the HTML file\n",
    "html_path = \"correlation_heatmap.html\"\n",
    "\n",
    "# Open the HTML file in a new tab\n",
    "webbrowser.open_new_tab(html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_bins = [0, 12, 24, 36, 48, 60, 72]\n",
    "tenure_labels = [\"0-1 Year\", \"1-2 Year\", \"2-3 Year\", \"3-4 Year\", \"4-5 Year\", \"5-6 Year\"]\n",
    "df = df.withColumn(\"NEW_TENURE_YEAR\", when((df[\"tenure\"] >= 0) & (df[\"tenure\"] <= 72), \"0-1 Year\").otherwise(None))\n",
    "for i in range(1, len(tenure_bins)):\n",
    "    condition = (df[\"tenure\"] > tenure_bins[i-1]) & (df[\"tenure\"] <= tenure_bins[i])\n",
    "    df = df.withColumn(\"NEW_TENURE_YEAR\", when(condition, tenure_labels[i-1]).otherwise(df[\"NEW_TENURE_YEAR\"]))\n",
    "\n",
    "# Specify contract 1 or 2 year customers as Engaged\n",
    "df = df.withColumn(\"NEW_Engaged\", when(df[\"Contract\"].isin([\"One year\", \"Two year\"]), 1).otherwise(0))\n",
    "\n",
    "# Customer benefit from at least one online support\n",
    "df = df.withColumn(\"NEW_noProt\", when((col(\"OnlineBackup\") != \"No\") | (col(\"DeviceProtection\") != \"No\") | (col(\"TechSupport\") != \"No\"), 1).otherwise(0))\n",
    "\n",
    "# Young customers with monthly contracts\n",
    "df = df.withColumn(\"NEW_Young_Not_Engaged\", when((col(\"NEW_Engaged\") == 0) & (col(\"SeniorCitizen\") == 0), 1).otherwise(0))\n",
    "\n",
    "# The total number of services received by the person\n",
    "service_columns = ['PhoneService', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df = df.withColumn(\"NEW_TotalServices\", sum(when(col(service_col) == \"Yes\", 1).otherwise(0) for service_col in service_columns))\n",
    "\n",
    "# Herhangi bir streaming hizmeti alan kişiler\n",
    "df = df.withColumn(\"NEW_FLAG_ANY_STREAMING\", when((col(\"StreamingTV\") == \"Yes\") | (col(\"StreamingMovies\") == \"Yes\"), 1).otherwise(0))\n",
    "\n",
    "# Does the person make automatic payments?\n",
    "df = df.withColumn(\"NEW_FLAG_AutoPayment\", when(col(\"PaymentMethod\").isin([\"Bank transfer (automatic)\", \"Credit card (automatic)\"]), 1).otherwise(0))\n",
    "\n",
    "# average monthly payment\n",
    "df = df.withColumn(\"NEW_AVG_Charges\", col(\"TotalCharges\") / (col(\"tenure\") + 0.1))\n",
    "\n",
    "# Sex and SeniorCitizen category\n",
    "df = df.withColumn(\"new_sex_cat\", when((col(\"gender\") == \"Male\") & (col(\"SeniorCitizen\") == 0), \"youngmale\")\n",
    "                  .when((col(\"gender\") == \"Male\") & (col(\"SeniorCitizen\") == 1), \"oldmale\")\n",
    "                  .when((col(\"gender\") == \"Female\") & (col(\"SeniorCitizen\") == 0), \"youngfemale\")\n",
    "                  .when((col(\"gender\") == \"Female\") & (col(\"SeniorCitizen\") == 1), \"oldfemale\")\n",
    "                  .otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|NEW_TENURE_YEAR|count|\n",
      "+---------------+-----+\n",
      "|       0-1 Year| 2186|\n",
      "|       1-2 Year| 1024|\n",
      "|       2-3 Year|  832|\n",
      "|       3-4 Year|  762|\n",
      "|       4-5 Year|  832|\n",
      "|       5-6 Year| 1407|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming `df` is your PySpark DataFrame\n",
    "value_counts_df = df.groupBy(\"NEW_TENURE_YEAR\").count().orderBy(\"NEW_TENURE_YEAR\")\n",
    "\n",
    "# Show the count of each unique value\n",
    "value_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"customerID\" column\n",
    "df = df.drop(\"customerID\")\n",
    "df = df.drop(\"converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [col for (col, data_type) in df.dtypes if data_type == 'string'and col != 'Churn']\n",
    "numerical_cols = [col for (col, data_type) in df.dtypes if data_type != 'string']\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "stages = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    string_indexer = StringIndexer(inputCol=col, outputCol=col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[col + \"_encoded\"])\n",
    "    stages += [string_indexer, encoder]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler_inputs = [col + \"_encoded\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Label indexing\n",
    "label_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "stages += [label_indexer]\n",
    "\n",
    "# Combine all stages into a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "encoded_df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Train-test split\n",
    "(train_data, test_data) = encoded_df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC RF: 0.8334650212606687\n",
      "Area Under ROC GBT: 0.8395521224281862\n"
     ]
    }
   ],
   "source": [
    "def base_models():\n",
    "    # Create RandomForestClassifier and GBTClassifier without hyperparameter tuning\n",
    "    rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    gbt_classifier = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    # Create pipelines\n",
    "    pipeline_rf = Pipeline(stages=[rf_classifier])\n",
    "    pipeline_gbt = Pipeline(stages=[gbt_classifier])\n",
    "\n",
    "    # Fit the models\n",
    "    model_rf = pipeline_rf.fit(train_data)\n",
    "    model_gbt = pipeline_gbt.fit(train_data)\n",
    "\n",
    "    # Return the models\n",
    "    return model_rf, model_gbt\n",
    "\n",
    "# Use base models\n",
    "base_model_rf, base_model_gbt = base_models()\n",
    "\n",
    "# Make predictions\n",
    "predictions_base_rf = base_model_rf.transform(test_data)\n",
    "predictions_base_gbt = base_model_gbt.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "print(\"Base Random Forest Predictions:\")\n",
    "predictions_base_rf.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "print(\"Base GBT Predictions:\")\n",
    "predictions_base_gbt.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions_base_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC RF: {auc_rf}\")\n",
    "\n",
    "auc_gbt = evaluator.evaluate(predictions_base_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC GBT: {auc_gbt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: True\n",
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: auto\n",
      "featuresCol: features\n",
      "impurity: gini\n",
      "labelCol: label\n",
      "leafCol: \n",
      "maxBins: 32\n",
      "maxDepth: 5\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "minWeightFractionPerNode: 0.0\n",
      "numTrees: 20\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: -4713376035466859559\n",
      "subsamplingRate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Access the model parameters\n",
    "params = base_model_rf.stages[0].extractParamMap()\n",
    "for key, value in params.items():\n",
    "    print(f\"{key.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC RF: 0.8407633048125\n",
      "Area Under ROC GBT: 0.8365410606422903\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_tuning(fold=3):\n",
    "    # Create RandomForestClassifier and GBTClassifier\n",
    "    rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    gbt_classifier = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    # Create a pipeline with StringIndexer, VectorAssembler, and the classifiers\n",
    "    pipeline_rf = Pipeline(stages=[rf_classifier])\n",
    "    pipeline_gbt = Pipeline(stages=[gbt_classifier])\n",
    "\n",
    "    # Create a range for maxDepth from 5 to 20 with a step size of 5\n",
    "    numTrees = [50, 100]\n",
    "    maxDepth = list(range(10, 20, 5))\n",
    "    maxBins = list(range(4, 20, 4))\n",
    "    minInstancesPerNode = [2, 4, 6, 8]\n",
    "\n",
    "    # Set up parameter grids for tuning\n",
    "    paramGrid_rf = (ParamGridBuilder()\n",
    "                    .addGrid(rf_classifier.numTrees, numTrees)\n",
    "                    .addGrid(rf_classifier.maxDepth, maxDepth)\n",
    "                    .addGrid(rf_classifier.maxBins, maxBins)\n",
    "                    .addGrid(rf_classifier.minInstancesPerNode, minInstancesPerNode)\n",
    "                    .build())\n",
    "\n",
    "    paramGrid_gbt = (ParamGridBuilder()\n",
    "                     .addGrid(gbt_classifier.maxIter, numTrees)\n",
    "                     .addGrid(gbt_classifier.maxDepth, maxDepth)\n",
    "                     .addGrid(gbt_classifier.maxBins, maxBins)\n",
    "                     .addGrid(gbt_classifier.minInstancesPerNode, minInstancesPerNode)\n",
    "                     .build())\n",
    "\n",
    "    # Create Search instances\n",
    "    search_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                                  estimatorParamMaps=paramGrid_rf,\n",
    "                                  evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                                  numFolds=fold)\n",
    "\n",
    "    search_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                                   estimatorParamMaps=paramGrid_gbt,\n",
    "                                   evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n",
    "                                   numFolds=fold)\n",
    "\n",
    "    # Fit the models\n",
    "    model_rf = search_rf.fit(train_data)\n",
    "    model_gbt = search_gbt.fit(train_data)\n",
    "\n",
    "    # Return the models\n",
    "    return model_rf, model_gbt\n",
    "\n",
    "model_rf, model_gbt = hyperparameter_tuning()\n",
    "\n",
    "# Make predictions\n",
    "predictions_rf = model_rf.transform(test_data)\n",
    "predictions_gbt = model_gbt.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "print(\"Random Forest Predictions:\")\n",
    "predictions_rf.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "print(\"GBT Predictions:\")\n",
    "predictions_gbt.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC RF: {auc_rf}\")\n",
    "\n",
    "auc_gbt = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC GBT: {auc_gbt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: all\n",
      "featuresCol: features\n",
      "impurity: variance\n",
      "labelCol: label\n",
      "leafCol: \n",
      "lossType: logistic\n",
      "maxBins: 8\n",
      "maxDepth: 5\n",
      "maxIter: 10\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 14\n",
      "minWeightFractionPerNode: 0.0\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: 3859683544350349931\n",
      "stepSize: 0.1\n",
      "subsamplingRate: 1.0\n",
      "validationTol: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Access the best model and its parameters\n",
    "bestModel = model_gbt.bestModel\n",
    "bestParams = bestModel.stages[0].extractParamMap()\n",
    "\n",
    "# Print the best parameters\n",
    "for key, value in bestParams.items():\n",
    "    print(f\"{key.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: True\n",
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: auto\n",
      "featuresCol: features\n",
      "impurity: gini\n",
      "labelCol: label\n",
      "leafCol: \n",
      "maxBins: 8\n",
      "maxDepth: 15\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 10\n",
      "minWeightFractionPerNode: 0.0\n",
      "numTrees: 50\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "seed: -4713376035466859559\n",
      "subsamplingRate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Access the best model and its parameters\n",
    "bestModel = model_rf.bestModel\n",
    "bestParams = bestModel.stages[0].extractParamMap()\n",
    "\n",
    "# Print the best parameters\n",
    "for key, value in bestParams.items():\n",
    "    print(f\"{key.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+----------------+\n",
      "|            features|label|combined_prediction|final_prediction|\n",
      "+--------------------+-----+-------------------+----------------+\n",
      "|(45,[1,2,7,8,10,1...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,7,8,10,1...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,7,8,10,1...|  0.0|                0.5|               1|\n",
      "|(45,[1,2,7,8,10,1...|  0.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,9,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,9,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,7,9,11,1...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,9,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,7,9,...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,9,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,9,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,7,8,10,1...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,7,8,10,1...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,20,2...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,23,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,7,8,10,1...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                0.5|               1|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,7,8,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,7,9,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,3,4,20,2...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  0.0|                1.0|               1|\n",
      "|(45,[1,2,7,8,11,1...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,7,9,...|  1.0|                0.0|               0|\n",
      "|(45,[1,2,3,4,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,7,9,...|  0.0|                0.0|               0|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,8,...|  1.0|                1.0|               1|\n",
      "|(45,[1,2,3,5,6,9,...|  1.0|                1.0|               1|\n",
      "+--------------------+-----+-------------------+----------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "# Alias prediction columns to avoid ambiguity\n",
    "predictions_rf = predictions_rf.withColumnRenamed(\"prediction\", \"prediction_rf\")\n",
    "predictions_gbt = predictions_gbt.withColumnRenamed(\"prediction\", \"prediction_gbt\")\n",
    "\n",
    "# Combine predictions without using withColumn or F\n",
    "combined_predictions = (\n",
    "    predictions_rf.join(\n",
    "        predictions_gbt.select(\"prediction_gbt\", \"features\", \"label\"),\n",
    "        on=\"features\"\n",
    "    )\n",
    "    .select(\n",
    "        predictions_rf[\"features\"],\n",
    "        predictions_rf[\"label\"],\n",
    "        ((col(\"prediction_rf\") + col(\"prediction_gbt\")) / 2.0).alias(\"combined_prediction\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_prediction\",\n",
    "        when(col(\"combined_prediction\") < 0.5, 0).otherwise(1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the combined predictions with the final prediction\n",
    "combined_predictions.select(\"features\", \"label\", \"combined_prediction\", \"final_prediction\").show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
